# RFC: 热备同步机制架构设计

## 介绍

为了解决 Master Service 故障后恢复时所有元数据丢失的问题，本方案提出了一种**热备同步机制**以及**热备与快照相结合的混合方案**，确保在 Master Service 故障后能够快速恢复数据。

## 动机

当前的 HA（高可用）模式仅支持 Master Service 故障后的快速重启，但无法保证数据的快速恢复。当前 HA 模式的痛点如下：

| 问题 | 影响 | 严重程度 |
|------|------|---------|
| **Leader 故障后元数据完全丢失** | 所有 KV Cache 位置信息丢失，必须重新构建 | 🔴 严重 |
| **Follower 缺乏数据预热** | 选举成功后，新的 Leader 必须从零开始提供服务 | 🟠 中等 |
| **恢复时间长** | 依赖客户端重新注册，可能需要几分钟到几十分钟 | 🟠 中等 |
| **服务不可用窗口** | Leader 故障切换期间无法处理请求 | 🟡 低 |

## 目标

1. **近零 RPO**：最小化数据丢失窗口
2. **快速故障切换**：实现秒级 RTO（恢复时间目标）
3. **数据一致性保证**：确保主备之间的正确同步
4. **可验证性**：包含验证数据完整性的机制
5. **低性能开销**：避免影响正常业务操作

## 提案

### 架构

我们提出了一种基于热备模型的架构，如下图所示：

<img width="1408" height="1213" alt="Image" src="https://github.com/user-attachments/assets/9589a524-4f8e-4080-8ce2-5b316502d422" />

### 描述

热备架构由四个主要层次协同工作，提供高可用性：

#### 第一层：Master 集群

Master 集群包含一个**主 Master**（Leader）和一个或多个**备 Master**（热备）。

**主 Master（Leader）**是活动节点，包含：

- **MasterService**：核心元数据管理组件，处理所有客户端请求（Query、Put、Remove）。它维护所有对象元数据的权威副本，包括副本位置、大小和租约信息。

- **OpLogManager**：为每个状态变更操作生成并缓冲操作日志（OpLog）条目。每个条目包含全局唯一的序列 ID、时间戳、操作类型、对象键和带校验和的有效载荷。

- **ReplicationService**：负责通过 gRPC 流将 OpLog 条目广播到所有连接的备节点。它跟踪每个备节点的确认，以监控复制延迟。

- **VerificationHandler**：响应来自备节点的定期验证请求，比较校验和以检测并帮助修复任何数据不一致。

**备 Master（热备）**是被动副本，包含：

- **HotStandbyService**：核心服务，管理备节点的生命周期，包括连接到主节点、协调复制和验证，以及在当选为新 Leader 时处理提升。

- **OpLogApplier**：从主节点接收 OpLog 条目，并按顺序将其应用到本地 MetadataStore。确保副本与主节点保持同步。

- **MetadataStore**：主节点元数据的完整副本。此副本使故障切换时能够即时提升，而不会丢失数据。

- **VerificationClient**：定期采样本地数据，计算校验和，并向主节点发送验证请求，以检测由 bug、网络问题或其他异常引起的不一致。

#### 第二层：协调层（etcd）

**etcd 集群**提供分布式协调：

- **Leader 选举**：使用 etcd 的租约机制，TTL 为 5 秒。主节点必须持续续约其租约；否则会在备节点中触发自动 Leader 选举。

- **服务发现**：存储当前 master 的地址，以便客户端发现并连接到活动的主节点。在故障切换期间原子更新。

#### 第三层：数据流

架构中有三个主要数据流：

1. **OpLog 复制流**（主 → 备）：
   - 步骤 1：客户端向 MasterService 发送写请求
   - 步骤 2：MasterService 处理请求并调用 OpLogManager.Append()
   - 步骤 3：OpLogManager 创建带 sequence_id 的条目并通知 ReplicationService
   - 步骤 4：ReplicationService 通过 gRPC 流将条目推送到所有备节点
   - 步骤 5：每个备节点的 OpLogApplier 将条目应用到 MetadataStore
   - 步骤 6：备节点向主节点发送 ACK

2. **验证流**（备 → 主）：
   - VerificationClient 从本地 MetadataStore 采样键
   - 计算 prefix_hash（键前缀的 CRC32）和 checksum（元数据的 CRC32）
   - 向主节点的 VerificationHandler 发送 VerificationRequest
   - 主节点与其数据进行比较并返回不匹配项
   - 备节点自动修复轻微的不一致

3. **客户端请求流**（客户端 → 主）：
   - 客户端查询 etcd 获取当前 master 地址
   - 向主节点的 MasterService 发送 RPC 请求（Query/Put/Remove）
   - 主节点处理请求并返回响应

#### 第四层：客户端层

**vLLM 推理集群**由多个 vLLM 实例组成，这些实例：

- 查询元数据以定位 KV Cache 副本
- 通过 Put 操作注册新的 KV Cache 对象
- 删除过期或未使用的对象

### 核心组件总结

| 组件 | 位置 | 职责 |
|------|------|------|
| **MasterService** | 主节点 | 处理所有客户端 RPC，管理元数据 |
| **OpLogManager** | 主节点 | 生成、缓冲和管理操作日志 |
| **ReplicationService** | 主节点 | 将 OpLog 推送到备节点，处理验证请求 |
| **VerificationHandler** | 主节点 | 响应备节点的验证请求 |
| **HotStandbyService** | 备节点 | 管理备节点生命周期和提升 |
| **OpLogApplier** | 备节点 | 将 OpLog 条目应用到本地元数据存储 |
| **MetadataStore** | 备节点 | 存储所有元数据的副本 |
| **VerificationClient** | 备节点 | 定期验证与主节点的数据一致性 |

### 关键设计决策

| 决策 | 理由 |
|------|------|
| **异步复制** | 最小化写延迟影响，同时提供近实时同步（~10ms） |
| **基于流的推送** | 比轮询更高效；能够立即传播变更 |
| **用于验证的前缀哈希** | 减少带宽（4 字节 vs 可变长度键），同时支持快速查找 |
| **校验和排除动态字段** | lease_timeout 和 soft_pin_timeout 频繁变化；排除它们可确保稳定的校验和 |
| **提升时的安全窗口** | 等待旧租约 TTL 可防止脑裂场景 |

---

## 对比

结合 Issue #1150 中的快照方案，我们分析了热备模式和快照模式的优缺点，如下所示：

| 指标 | 快照模式 | 热备模式 | 胜者 |
|------|---------|---------|------|
| **RPO** | 5–30 分钟 | <10 秒 | 热备模式 |
| **RTO（1000 万条目）** | 随着数据量增加，RTO 会越来越大 | 3~10 秒 | 热备模式 |
| **写延迟开销** | 0% | <5% | 快照模式 |
| **CPU 开销（正常操作）** | 0% | ~5% | 快照模式 |
| **内存开销** | 0%（快照期间翻倍） | ~10% | 平局 |
| **网络带宽使用** | 0（快照期间突发） | 持续低带宽 | 平局 |
| **实现复杂度** | 低 | 高 | 快照模式 |
| **冷启动能力** | ✅ | ❌ | 快照模式 |
| **数据可验证性** | ❌ | ✅ | 热备模式 |
| **自动恢复** | ❌ | ✅ | 热备模式 |

## 推荐使用场景

热备模式提供实时数据复制和快速故障切换，而快照模式提供持久化回退和冷启动能力。两者结合，可在任何场景下实现零数据丢失和可恢复性。

将热备模式和快照模式相结合的模式定义为**混合模式**。

基于上述架构描述以及与 Issue #1150 的对比结果，提供以下应用场景建议：

- **简单场景或资源受限环境** → **快照模式**
- **生产环境** → **热备模式**或**混合模式**
- **关键任务工作负载** → **混合模式**

